{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Optical Character Recognition\n",
    "\n",
    "### 4.2.1 Task 1 : Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'words.txt' has been created with 100000 words.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import nltk  # type: ignore\n",
    "\n",
    "# nltk.download('words')\n",
    "from nltk.corpus import words  # type: ignore\n",
    "\n",
    "# Create the dataset\n",
    "\n",
    "def create_dataset():\n",
    "\n",
    "    # Set up image and font parameters\n",
    "    img_width = 256\n",
    "    img_height = 64\n",
    "\n",
    "    # Path to a valid font file \n",
    "    font_path = \"/Library/Fonts/Arial.ttf\"  \n",
    "    font_size = 16.5  # Base font size\n",
    "    font = ImageFont.truetype(font_path, int(font_size)) \n",
    "\n",
    "    \n",
    "    output_dir = \"../../data/interim/5/OCR_dataset/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    all_words = words.words() \n",
    "    selected_words = all_words[:100000]  \n",
    "    for i, word in enumerate(selected_words):\n",
    "        img = Image.new('RGB', (img_width, img_height), color=(255, 255, 255))  # Create a white image\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Calculate the position to center the word\n",
    "        bbox = draw.textbbox((0, 0), word, font=font)  # Get bounding box of the text\n",
    "        text_width = bbox[2] - bbox[0]  \n",
    "        text_height = bbox[3] - bbox[1] \n",
    "        text_x = (img_width - text_width) // 2\n",
    "        text_y = (img_height - text_height) // 2\n",
    "        \n",
    "        # Draw the word onto the image\n",
    "        draw.text((text_x, text_y), word, font=font, fill=(0, 0, 0)) \n",
    "        \n",
    "        img.save(f\"{output_dir}word_{i}.png\") \n",
    "\n",
    "    print(\"Dataset creation completed.\")\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "all_words = words.words()\n",
    "selected_words = all_words[:100000]\n",
    "\n",
    "# Write these words to a text file\n",
    "output_file = \"words.txt\"\n",
    "\n",
    "# with open(output_file, \"w\") as file:\n",
    "#     for word in selected_words:\n",
    "#         file.write(word + \"\\n\")\n",
    "\n",
    "print(f\"File '{output_file}' has been created with {len(selected_words)} words.\")\n",
    "\n",
    "# create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Task 2 : Architecture \n",
    "\n",
    "### 4.2.3 Task 3 : Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 64\n",
    "BATCH_SIZE = 32\n",
    "MAX_WORD_LENGTH = 30\n",
    "HIDDEN_SIZE = 256\n",
    "DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Character mapping\n",
    "ALL_CHARS = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "CHAR_TO_IDX = {char: idx + 1 for idx, char in enumerate(ALL_CHARS)}\n",
    "IDX_TO_CHAR = {idx + 1: char for idx, char in enumerate(ALL_CHARS)}\n",
    "VOCAB_SIZE = len(CHAR_TO_IDX) + 1\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, image_dir, word_list_path, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        with open(word_list_path, 'r') as f:\n",
    "            self.labels = []\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if len(word) <= MAX_WORD_LENGTH:\n",
    "                    self.labels.append(word)\n",
    "        self.labels = self.labels[:100000]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, f\"word_{idx}.png\")\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        word = self.labels[idx].lower()\n",
    "        label = torch.zeros(MAX_WORD_LENGTH, dtype=torch.long)\n",
    "        for i, char in enumerate(word):\n",
    "            if char in CHAR_TO_IDX:\n",
    "                label[i] = CHAR_TO_IDX[char]\n",
    "        \n",
    "        return image, label, len(word)\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        # Simplified CNN architecture with careful dimension handling\n",
    "        self.features = nn.Sequential(\n",
    "            # Input: batch x 1 x 64 x 256\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # 64 x 64 x 256\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 64 x 32 x 128\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # 128 x 32 x 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 128 x 16 x 64\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # 256 x 16 x 64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)),  # 256 x 8 x 32\n",
    "            \n",
    "            nn.Dropout2d(DROPOUT_RATE)\n",
    "        )\n",
    "        \n",
    "        # Calculate the exact output size\n",
    "        self.feature_size = 256 * 8  # Height after CNN\n",
    "        self.sequence_length = 32     # Width after CNN\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: batch x 1 x 64 x 256\n",
    "        x = self.features(x)  # batch x 256 x 8 x 32\n",
    "        \n",
    "        # Reshape to (batch, sequence_length, features)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0, 3, 1, 2)  # batch x 32 x 256 x 8\n",
    "        x = x.contiguous().view(batch_size, self.sequence_length, -1)  # batch x 32 x (256*8)\n",
    "        \n",
    "        \n",
    "        x = x[:, :MAX_WORD_LENGTH, :]\n",
    "        return x\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class OCRModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OCRModel, self).__init__()\n",
    "        self.encoder = CNNEncoder()\n",
    "        self.decoder = RNNDecoder(\n",
    "            input_size=256 * 8,  # Matches encoder output size\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            output_size=VOCAB_SIZE\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    for batch_idx, (images, labels, lengths) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = 0\n",
    "        for t in range(MAX_WORD_LENGTH):\n",
    "            loss += criterion(outputs[:, t, :], labels[:, t])\n",
    "        \n",
    "        loss = loss / MAX_WORD_LENGTH\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(train_loader), batch_losses\n",
    "\n",
    "def plot_training_progress(train_losses, val_losses, save_path='training_progress.png'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "Dataset size: 100000\n",
      "Epoch: 1, Batch: 0/2500, Loss: 1.6456\n",
      "Epoch: 1, Batch: 50/2500, Loss: 1.4444\n",
      "Epoch: 1, Batch: 100/2500, Loss: 1.2795\n",
      "Epoch: 1, Batch: 150/2500, Loss: 1.5007\n",
      "Epoch: 1, Batch: 200/2500, Loss: 1.3646\n",
      "Epoch: 1, Batch: 250/2500, Loss: 1.2748\n",
      "Epoch: 1, Batch: 300/2500, Loss: 1.5434\n",
      "Epoch: 1, Batch: 350/2500, Loss: 1.4867\n",
      "Epoch: 1, Batch: 400/2500, Loss: 1.4037\n",
      "Epoch: 1, Batch: 450/2500, Loss: 1.6456\n",
      "Epoch: 1, Batch: 500/2500, Loss: 1.4856\n",
      "Epoch: 1, Batch: 550/2500, Loss: 1.4267\n",
      "Epoch: 1, Batch: 600/2500, Loss: 1.4955\n",
      "Epoch: 1, Batch: 650/2500, Loss: 1.5592\n",
      "Epoch: 1, Batch: 700/2500, Loss: 1.1766\n",
      "Epoch: 1, Batch: 750/2500, Loss: 1.3743\n",
      "Epoch: 1, Batch: 800/2500, Loss: 1.2482\n",
      "Epoch: 1, Batch: 850/2500, Loss: 1.2720\n",
      "Epoch: 1, Batch: 900/2500, Loss: 1.6634\n",
      "Epoch: 1, Batch: 950/2500, Loss: 1.8340\n",
      "Epoch: 1, Batch: 1000/2500, Loss: 1.3984\n",
      "Epoch: 1, Batch: 1050/2500, Loss: 1.8862\n",
      "Epoch: 1, Batch: 1100/2500, Loss: 1.4689\n",
      "Epoch: 1, Batch: 1150/2500, Loss: 1.4729\n",
      "Epoch: 1, Batch: 1200/2500, Loss: 1.3006\n",
      "Epoch: 1, Batch: 1250/2500, Loss: 1.1615\n",
      "Epoch: 1, Batch: 1300/2500, Loss: 1.3138\n",
      "Epoch: 1, Batch: 1350/2500, Loss: 1.1435\n",
      "Epoch: 1, Batch: 1400/2500, Loss: 1.2498\n",
      "Epoch: 1, Batch: 1450/2500, Loss: 1.3917\n",
      "Epoch: 1, Batch: 1500/2500, Loss: 1.3515\n",
      "Epoch: 1, Batch: 1550/2500, Loss: 1.6600\n",
      "Epoch: 1, Batch: 1600/2500, Loss: 1.2646\n",
      "Epoch: 1, Batch: 1650/2500, Loss: 1.2833\n",
      "Epoch: 1, Batch: 1700/2500, Loss: 1.3236\n",
      "Epoch: 1, Batch: 1750/2500, Loss: 1.4221\n",
      "Epoch: 1, Batch: 1800/2500, Loss: 1.3016\n",
      "Epoch: 1, Batch: 1850/2500, Loss: 1.1559\n",
      "Epoch: 1, Batch: 1900/2500, Loss: 1.1167\n",
      "Epoch: 1, Batch: 1950/2500, Loss: 1.4396\n",
      "Epoch: 1, Batch: 2000/2500, Loss: 1.1976\n",
      "Epoch: 1, Batch: 2050/2500, Loss: 1.3158\n",
      "Epoch: 1, Batch: 2100/2500, Loss: 1.0860\n",
      "Epoch: 1, Batch: 2150/2500, Loss: 1.2570\n",
      "Epoch: 1, Batch: 2200/2500, Loss: 1.0690\n",
      "Epoch: 1, Batch: 2250/2500, Loss: 1.0443\n",
      "Epoch: 1, Batch: 2300/2500, Loss: 1.1122\n",
      "Epoch: 1, Batch: 2350/2500, Loss: 1.4061\n",
      "Epoch: 1, Batch: 2400/2500, Loss: 1.1240\n",
      "Epoch: 1, Batch: 2450/2500, Loss: 1.1185\n",
      "Epoch 1: Train Loss: 1.3782, Val Loss: 1.3801\n",
      "Saved best model with validation loss: 1.3801\n",
      "Epoch: 2, Batch: 0/2500, Loss: 1.5989\n",
      "Epoch: 2, Batch: 50/2500, Loss: 1.4139\n",
      "Epoch: 2, Batch: 100/2500, Loss: 0.9922\n",
      "Epoch: 2, Batch: 150/2500, Loss: 1.4120\n",
      "Epoch: 2, Batch: 200/2500, Loss: 1.1462\n",
      "Epoch: 2, Batch: 250/2500, Loss: 1.1289\n",
      "Epoch: 2, Batch: 300/2500, Loss: 1.1035\n",
      "Epoch: 2, Batch: 350/2500, Loss: 1.0069\n",
      "Epoch: 2, Batch: 400/2500, Loss: 1.0222\n",
      "Epoch: 2, Batch: 450/2500, Loss: 1.1792\n",
      "Epoch: 2, Batch: 500/2500, Loss: 1.1445\n",
      "Epoch: 2, Batch: 550/2500, Loss: 0.9823\n",
      "Epoch: 2, Batch: 600/2500, Loss: 0.8748\n",
      "Epoch: 2, Batch: 650/2500, Loss: 1.0043\n",
      "Epoch: 2, Batch: 700/2500, Loss: 0.9939\n",
      "Epoch: 2, Batch: 750/2500, Loss: 1.2039\n",
      "Epoch: 2, Batch: 800/2500, Loss: 1.4860\n",
      "Epoch: 2, Batch: 850/2500, Loss: 0.9887\n",
      "Epoch: 2, Batch: 900/2500, Loss: 1.2019\n",
      "Epoch: 2, Batch: 950/2500, Loss: 1.1808\n",
      "Epoch: 2, Batch: 1000/2500, Loss: 1.0094\n",
      "Epoch: 2, Batch: 1050/2500, Loss: 1.3296\n",
      "Epoch: 2, Batch: 1100/2500, Loss: 0.9681\n",
      "Epoch: 2, Batch: 1150/2500, Loss: 1.2826\n",
      "Epoch: 2, Batch: 1200/2500, Loss: 0.6279\n",
      "Epoch: 2, Batch: 1250/2500, Loss: 1.5030\n",
      "Epoch: 2, Batch: 1300/2500, Loss: 1.7065\n",
      "Epoch: 2, Batch: 1350/2500, Loss: 1.1530\n",
      "Epoch: 2, Batch: 1400/2500, Loss: 1.0890\n",
      "Epoch: 2, Batch: 1450/2500, Loss: 1.3020\n",
      "Epoch: 2, Batch: 1500/2500, Loss: 1.4021\n",
      "Epoch: 2, Batch: 1550/2500, Loss: 1.0026\n",
      "Epoch: 2, Batch: 1600/2500, Loss: 1.1332\n",
      "Epoch: 2, Batch: 1650/2500, Loss: 1.3730\n",
      "Epoch: 2, Batch: 1700/2500, Loss: 0.8880\n",
      "Epoch: 2, Batch: 1750/2500, Loss: 0.9821\n",
      "Epoch: 2, Batch: 1800/2500, Loss: 0.9178\n",
      "Epoch: 2, Batch: 1850/2500, Loss: 0.9999\n",
      "Epoch: 2, Batch: 1900/2500, Loss: 0.9536\n",
      "Epoch: 2, Batch: 1950/2500, Loss: 0.8561\n",
      "Epoch: 2, Batch: 2000/2500, Loss: 0.8291\n",
      "Epoch: 2, Batch: 2050/2500, Loss: 1.0304\n",
      "Epoch: 2, Batch: 2100/2500, Loss: 1.0213\n",
      "Epoch: 2, Batch: 2150/2500, Loss: 0.9309\n",
      "Epoch: 2, Batch: 2200/2500, Loss: 0.9469\n",
      "Epoch: 2, Batch: 2250/2500, Loss: 0.7229\n",
      "Epoch: 2, Batch: 2300/2500, Loss: 1.2276\n",
      "Epoch: 2, Batch: 2350/2500, Loss: 0.9906\n",
      "Epoch: 2, Batch: 2400/2500, Loss: 0.8941\n",
      "Epoch: 2, Batch: 2450/2500, Loss: 0.9088\n",
      "Epoch 2: Train Loss: 1.1173, Val Loss: 1.0039\n",
      "Saved best model with validation loss: 1.0039\n",
      "Epoch: 3, Batch: 0/2500, Loss: 0.9652\n",
      "Epoch: 3, Batch: 50/2500, Loss: 0.9280\n",
      "Epoch: 3, Batch: 100/2500, Loss: 1.0750\n",
      "Epoch: 3, Batch: 150/2500, Loss: 0.8832\n",
      "Epoch: 3, Batch: 200/2500, Loss: 1.1070\n",
      "Epoch: 3, Batch: 250/2500, Loss: 0.8345\n",
      "Epoch: 3, Batch: 300/2500, Loss: 0.9923\n",
      "Epoch: 3, Batch: 350/2500, Loss: 0.8133\n",
      "Epoch: 3, Batch: 400/2500, Loss: 0.9996\n",
      "Epoch: 3, Batch: 450/2500, Loss: 0.9402\n",
      "Epoch: 3, Batch: 500/2500, Loss: 0.8089\n",
      "Epoch: 3, Batch: 550/2500, Loss: 0.9072\n",
      "Epoch: 3, Batch: 600/2500, Loss: 1.0359\n",
      "Epoch: 3, Batch: 650/2500, Loss: 1.1214\n",
      "Epoch: 3, Batch: 700/2500, Loss: 0.9939\n",
      "Epoch: 3, Batch: 750/2500, Loss: 1.4357\n",
      "Epoch: 3, Batch: 800/2500, Loss: 1.2842\n",
      "Epoch: 3, Batch: 850/2500, Loss: 0.7838\n",
      "Epoch: 3, Batch: 900/2500, Loss: 0.7709\n",
      "Epoch: 3, Batch: 950/2500, Loss: 0.8891\n",
      "Epoch: 3, Batch: 1000/2500, Loss: 0.9515\n",
      "Epoch: 3, Batch: 1050/2500, Loss: 1.2737\n",
      "Epoch: 3, Batch: 1100/2500, Loss: 1.0084\n",
      "Epoch: 3, Batch: 1150/2500, Loss: 1.0565\n",
      "Epoch: 3, Batch: 1200/2500, Loss: 0.8879\n",
      "Epoch: 3, Batch: 1250/2500, Loss: 0.8302\n",
      "Epoch: 3, Batch: 1300/2500, Loss: 0.8958\n",
      "Epoch: 3, Batch: 1350/2500, Loss: 0.9553\n",
      "Epoch: 3, Batch: 1400/2500, Loss: 1.0015\n",
      "Epoch: 3, Batch: 1450/2500, Loss: 0.6961\n",
      "Epoch: 3, Batch: 1500/2500, Loss: 0.8686\n",
      "Epoch: 3, Batch: 1550/2500, Loss: 0.8983\n",
      "Epoch: 3, Batch: 1600/2500, Loss: 0.8326\n",
      "Epoch: 3, Batch: 1650/2500, Loss: 0.7599\n",
      "Epoch: 3, Batch: 1700/2500, Loss: 0.8914\n",
      "Epoch: 3, Batch: 1750/2500, Loss: 1.0842\n",
      "Epoch: 3, Batch: 1800/2500, Loss: 0.8482\n",
      "Epoch: 3, Batch: 1850/2500, Loss: 0.7184\n",
      "Epoch: 3, Batch: 1900/2500, Loss: 0.8075\n",
      "Epoch: 3, Batch: 1950/2500, Loss: 1.2174\n",
      "Epoch: 3, Batch: 2000/2500, Loss: 1.1536\n",
      "Epoch: 3, Batch: 2050/2500, Loss: 0.9728\n",
      "Epoch: 3, Batch: 2100/2500, Loss: 0.7798\n",
      "Epoch: 3, Batch: 2150/2500, Loss: 0.7985\n",
      "Epoch: 3, Batch: 2200/2500, Loss: 0.6884\n",
      "Epoch: 3, Batch: 2250/2500, Loss: 1.0064\n",
      "Epoch: 3, Batch: 2300/2500, Loss: 0.8321\n",
      "Epoch: 3, Batch: 2350/2500, Loss: 0.6244\n",
      "Epoch: 3, Batch: 2400/2500, Loss: 0.7507\n",
      "Epoch: 3, Batch: 2450/2500, Loss: 1.0260\n",
      "Epoch 3: Train Loss: 0.9307, Val Loss: 0.7896\n",
      "Saved best model with validation loss: 0.7896\n",
      "Epoch: 4, Batch: 0/2500, Loss: 1.2964\n",
      "Epoch: 4, Batch: 50/2500, Loss: 0.5898\n",
      "Epoch: 4, Batch: 100/2500, Loss: 0.7081\n",
      "Epoch: 4, Batch: 150/2500, Loss: 0.7558\n",
      "Epoch: 4, Batch: 200/2500, Loss: 0.5348\n",
      "Epoch: 4, Batch: 250/2500, Loss: 0.8896\n",
      "Epoch: 4, Batch: 300/2500, Loss: 1.0064\n",
      "Epoch: 4, Batch: 350/2500, Loss: 0.8635\n",
      "Epoch: 4, Batch: 400/2500, Loss: 0.7801\n",
      "Epoch: 4, Batch: 450/2500, Loss: 0.5862\n",
      "Epoch: 4, Batch: 500/2500, Loss: 0.6015\n",
      "Epoch: 4, Batch: 550/2500, Loss: 0.6817\n",
      "Epoch: 4, Batch: 600/2500, Loss: 0.7930\n",
      "Epoch: 4, Batch: 650/2500, Loss: 0.8869\n",
      "Epoch: 4, Batch: 700/2500, Loss: 0.6307\n",
      "Epoch: 4, Batch: 750/2500, Loss: 0.8811\n",
      "Epoch: 4, Batch: 800/2500, Loss: 0.7274\n",
      "Epoch: 4, Batch: 850/2500, Loss: 0.6361\n",
      "Epoch: 4, Batch: 900/2500, Loss: 0.8181\n",
      "Epoch: 4, Batch: 950/2500, Loss: 0.8130\n",
      "Epoch: 4, Batch: 1000/2500, Loss: 0.4222\n",
      "Epoch: 4, Batch: 1050/2500, Loss: 0.5159\n",
      "Epoch: 4, Batch: 1100/2500, Loss: 0.7807\n",
      "Epoch: 4, Batch: 1150/2500, Loss: 0.6942\n",
      "Epoch: 4, Batch: 1200/2500, Loss: 0.6114\n",
      "Epoch: 4, Batch: 1250/2500, Loss: 0.6201\n",
      "Epoch: 4, Batch: 1300/2500, Loss: 0.7020\n",
      "Epoch: 4, Batch: 1350/2500, Loss: 0.5167\n",
      "Epoch: 4, Batch: 1400/2500, Loss: 0.4632\n",
      "Epoch: 4, Batch: 1450/2500, Loss: 0.7648\n",
      "Epoch: 4, Batch: 1500/2500, Loss: 0.6306\n",
      "Epoch: 4, Batch: 1550/2500, Loss: 0.4441\n",
      "Epoch: 4, Batch: 1600/2500, Loss: 0.6017\n",
      "Epoch: 4, Batch: 1650/2500, Loss: 0.6387\n",
      "Epoch: 4, Batch: 1700/2500, Loss: 0.5377\n",
      "Epoch: 4, Batch: 1750/2500, Loss: 0.5720\n",
      "Epoch: 4, Batch: 1800/2500, Loss: 0.7286\n",
      "Epoch: 4, Batch: 1850/2500, Loss: 0.5514\n",
      "Epoch: 4, Batch: 1900/2500, Loss: 0.9113\n",
      "Epoch: 4, Batch: 1950/2500, Loss: 0.5071\n",
      "Epoch: 4, Batch: 2000/2500, Loss: 0.5867\n",
      "Epoch: 4, Batch: 2050/2500, Loss: 0.5532\n",
      "Epoch: 4, Batch: 2100/2500, Loss: 0.3469\n",
      "Epoch: 4, Batch: 2150/2500, Loss: 0.9680\n",
      "Epoch: 4, Batch: 2200/2500, Loss: 0.6734\n",
      "Epoch: 4, Batch: 2250/2500, Loss: 0.5434\n",
      "Epoch: 4, Batch: 2300/2500, Loss: 0.6107\n",
      "Epoch: 4, Batch: 2350/2500, Loss: 0.4858\n",
      "Epoch: 4, Batch: 2400/2500, Loss: 0.5201\n",
      "Epoch: 4, Batch: 2450/2500, Loss: 0.4400\n",
      "Epoch 4: Train Loss: 0.6743, Val Loss: 0.6508\n",
      "Saved best model with validation loss: 0.6508\n",
      "Epoch: 5, Batch: 0/2500, Loss: 0.6721\n",
      "Epoch: 5, Batch: 50/2500, Loss: 0.5856\n",
      "Epoch: 5, Batch: 100/2500, Loss: 0.6205\n",
      "Epoch: 5, Batch: 150/2500, Loss: 0.4363\n",
      "Epoch: 5, Batch: 200/2500, Loss: 0.5770\n",
      "Epoch: 5, Batch: 250/2500, Loss: 0.3380\n",
      "Epoch: 5, Batch: 300/2500, Loss: 0.5829\n",
      "Epoch: 5, Batch: 350/2500, Loss: 0.4271\n",
      "Epoch: 5, Batch: 400/2500, Loss: 0.8283\n",
      "Epoch: 5, Batch: 450/2500, Loss: 0.4073\n",
      "Epoch: 5, Batch: 500/2500, Loss: 0.7895\n",
      "Epoch: 5, Batch: 550/2500, Loss: 0.6508\n",
      "Epoch: 5, Batch: 600/2500, Loss: 0.5291\n",
      "Epoch: 5, Batch: 650/2500, Loss: 0.3618\n",
      "Epoch: 5, Batch: 700/2500, Loss: 0.3392\n",
      "Epoch: 5, Batch: 750/2500, Loss: 0.4026\n",
      "Epoch: 5, Batch: 800/2500, Loss: 0.5924\n",
      "Epoch: 5, Batch: 850/2500, Loss: 0.4742\n",
      "Epoch: 5, Batch: 900/2500, Loss: 0.4663\n",
      "Epoch: 5, Batch: 950/2500, Loss: 0.4940\n",
      "Epoch: 5, Batch: 1000/2500, Loss: 0.3979\n",
      "Epoch: 5, Batch: 1050/2500, Loss: 0.6457\n",
      "Epoch: 5, Batch: 1100/2500, Loss: 0.4443\n",
      "Epoch: 5, Batch: 1150/2500, Loss: 0.3145\n",
      "Epoch: 5, Batch: 1200/2500, Loss: 0.4711\n",
      "Epoch: 5, Batch: 1250/2500, Loss: 0.4584\n",
      "Epoch: 5, Batch: 1300/2500, Loss: 0.5686\n",
      "Epoch: 5, Batch: 1350/2500, Loss: 0.5368\n",
      "Epoch: 5, Batch: 1400/2500, Loss: 0.3505\n",
      "Epoch: 5, Batch: 1450/2500, Loss: 0.3475\n",
      "Epoch: 5, Batch: 1500/2500, Loss: 0.3071\n",
      "Epoch: 5, Batch: 1550/2500, Loss: 0.2919\n",
      "Epoch: 5, Batch: 1600/2500, Loss: 0.3380\n",
      "Epoch: 5, Batch: 1650/2500, Loss: 0.3872\n",
      "Epoch: 5, Batch: 1700/2500, Loss: 0.6014\n",
      "Epoch: 5, Batch: 1750/2500, Loss: 0.2861\n",
      "Epoch: 5, Batch: 1800/2500, Loss: 0.4661\n",
      "Epoch: 5, Batch: 1850/2500, Loss: 0.3165\n",
      "Epoch: 5, Batch: 1900/2500, Loss: 0.3099\n",
      "Epoch: 5, Batch: 1950/2500, Loss: 0.4392\n",
      "Epoch: 5, Batch: 2000/2500, Loss: 0.2561\n",
      "Epoch: 5, Batch: 2050/2500, Loss: 0.4716\n",
      "Epoch: 5, Batch: 2100/2500, Loss: 0.3810\n",
      "Epoch: 5, Batch: 2150/2500, Loss: 0.4240\n",
      "Epoch: 5, Batch: 2200/2500, Loss: 0.7511\n",
      "Epoch: 5, Batch: 2250/2500, Loss: 0.3017\n",
      "Epoch: 5, Batch: 2300/2500, Loss: 0.4851\n",
      "Epoch: 5, Batch: 2350/2500, Loss: 0.3377\n",
      "Epoch: 5, Batch: 2400/2500, Loss: 0.4459\n",
      "Epoch: 5, Batch: 2450/2500, Loss: 0.3304\n",
      "Epoch 5: Train Loss: 0.4581, Val Loss: 0.4033\n",
      "Saved best model with validation loss: 0.4033\n",
      "Epoch: 6, Batch: 0/2500, Loss: 0.2238\n",
      "Epoch: 6, Batch: 50/2500, Loss: 0.3483\n",
      "Epoch: 6, Batch: 100/2500, Loss: 0.2691\n",
      "Epoch: 6, Batch: 150/2500, Loss: 0.3001\n",
      "Epoch: 6, Batch: 200/2500, Loss: 0.4383\n",
      "Epoch: 6, Batch: 250/2500, Loss: 0.4080\n",
      "Epoch: 6, Batch: 300/2500, Loss: 0.4428\n",
      "Epoch: 6, Batch: 350/2500, Loss: 0.4291\n",
      "Epoch: 6, Batch: 400/2500, Loss: 0.2039\n",
      "Epoch: 6, Batch: 450/2500, Loss: 0.2281\n",
      "Epoch: 6, Batch: 500/2500, Loss: 0.5973\n",
      "Epoch: 6, Batch: 550/2500, Loss: 0.2362\n",
      "Epoch: 6, Batch: 600/2500, Loss: 0.2576\n",
      "Epoch: 6, Batch: 650/2500, Loss: 0.4637\n",
      "Epoch: 6, Batch: 700/2500, Loss: 0.3823\n",
      "Epoch: 6, Batch: 750/2500, Loss: 0.3153\n",
      "Epoch: 6, Batch: 800/2500, Loss: 0.4130\n",
      "Epoch: 6, Batch: 850/2500, Loss: 0.4662\n",
      "Epoch: 6, Batch: 900/2500, Loss: 0.4620\n",
      "Epoch: 6, Batch: 950/2500, Loss: 0.1863\n",
      "Epoch: 6, Batch: 1000/2500, Loss: 0.2731\n",
      "Epoch: 6, Batch: 1050/2500, Loss: 0.3387\n",
      "Epoch: 6, Batch: 1100/2500, Loss: 0.4929\n",
      "Epoch: 6, Batch: 1150/2500, Loss: 0.5441\n",
      "Epoch: 6, Batch: 1200/2500, Loss: 0.3765\n",
      "Epoch: 6, Batch: 1250/2500, Loss: 0.4239\n",
      "Epoch: 6, Batch: 1300/2500, Loss: 0.1979\n",
      "Epoch: 6, Batch: 1350/2500, Loss: 0.2482\n",
      "Epoch: 6, Batch: 1400/2500, Loss: 0.3078\n",
      "Epoch: 6, Batch: 1450/2500, Loss: 0.2687\n",
      "Epoch: 6, Batch: 1500/2500, Loss: 0.1743\n",
      "Epoch: 6, Batch: 1550/2500, Loss: 0.6558\n",
      "Epoch: 6, Batch: 1600/2500, Loss: 0.2696\n",
      "Epoch: 6, Batch: 1650/2500, Loss: 0.2519\n",
      "Epoch: 6, Batch: 1700/2500, Loss: 0.3759\n",
      "Epoch: 6, Batch: 1750/2500, Loss: 0.2357\n",
      "Epoch: 6, Batch: 1800/2500, Loss: 0.3243\n",
      "Epoch: 6, Batch: 1850/2500, Loss: 0.3454\n",
      "Epoch: 6, Batch: 1900/2500, Loss: 0.2266\n",
      "Epoch: 6, Batch: 1950/2500, Loss: 0.2878\n",
      "Epoch: 6, Batch: 2000/2500, Loss: 0.1938\n",
      "Epoch: 6, Batch: 2050/2500, Loss: 0.3664\n",
      "Epoch: 6, Batch: 2100/2500, Loss: 0.2763\n",
      "Epoch: 6, Batch: 2150/2500, Loss: 0.3082\n",
      "Epoch: 6, Batch: 2200/2500, Loss: 0.2361\n",
      "Epoch: 6, Batch: 2250/2500, Loss: 0.5126\n",
      "Epoch: 6, Batch: 2300/2500, Loss: 0.1821\n",
      "Epoch: 6, Batch: 2350/2500, Loss: 0.2113\n",
      "Epoch: 6, Batch: 2400/2500, Loss: 0.2199\n",
      "Epoch: 6, Batch: 2450/2500, Loss: 0.2200\n",
      "Epoch 6: Train Loss: 0.3439, Val Loss: 0.2867\n",
      "Saved best model with validation loss: 0.2867\n",
      "Epoch: 7, Batch: 0/2500, Loss: 0.1521\n",
      "Epoch: 7, Batch: 50/2500, Loss: 0.2252\n",
      "Epoch: 7, Batch: 100/2500, Loss: 0.2186\n",
      "Epoch: 7, Batch: 150/2500, Loss: 0.1781\n",
      "Epoch: 7, Batch: 200/2500, Loss: 0.2727\n",
      "Epoch: 7, Batch: 250/2500, Loss: 0.4645\n",
      "Epoch: 7, Batch: 300/2500, Loss: 0.1232\n",
      "Epoch: 7, Batch: 350/2500, Loss: 0.3907\n",
      "Epoch: 7, Batch: 400/2500, Loss: 0.2228\n",
      "Epoch: 7, Batch: 450/2500, Loss: 0.2179\n",
      "Epoch: 7, Batch: 500/2500, Loss: 0.3222\n",
      "Epoch: 7, Batch: 550/2500, Loss: 0.2148\n",
      "Epoch: 7, Batch: 600/2500, Loss: 0.2830\n",
      "Epoch: 7, Batch: 650/2500, Loss: 0.1829\n",
      "Epoch: 7, Batch: 700/2500, Loss: 0.1558\n",
      "Epoch: 7, Batch: 750/2500, Loss: 0.3984\n",
      "Epoch: 7, Batch: 800/2500, Loss: 0.2538\n",
      "Epoch: 7, Batch: 850/2500, Loss: 0.2085\n",
      "Epoch: 7, Batch: 900/2500, Loss: 0.1404\n",
      "Epoch: 7, Batch: 950/2500, Loss: 0.1558\n",
      "Epoch: 7, Batch: 1000/2500, Loss: 0.2697\n",
      "Epoch: 7, Batch: 1050/2500, Loss: 0.4784\n",
      "Epoch: 7, Batch: 1100/2500, Loss: 0.2220\n",
      "Epoch: 7, Batch: 1150/2500, Loss: 0.2951\n",
      "Epoch: 7, Batch: 1200/2500, Loss: 0.1912\n",
      "Epoch: 7, Batch: 1250/2500, Loss: 0.3313\n",
      "Epoch: 7, Batch: 1300/2500, Loss: 0.5424\n",
      "Epoch: 7, Batch: 1350/2500, Loss: 0.1529\n",
      "Epoch: 7, Batch: 1400/2500, Loss: 0.2136\n",
      "Epoch: 7, Batch: 1450/2500, Loss: 0.1186\n",
      "Epoch: 7, Batch: 1500/2500, Loss: 0.2258\n",
      "Epoch: 7, Batch: 1550/2500, Loss: 0.2634\n",
      "Epoch: 7, Batch: 1600/2500, Loss: 0.5689\n",
      "Epoch: 7, Batch: 1650/2500, Loss: 0.2604\n",
      "Epoch: 7, Batch: 1700/2500, Loss: 0.2375\n",
      "Epoch: 7, Batch: 1750/2500, Loss: 0.1742\n",
      "Epoch: 7, Batch: 1800/2500, Loss: 0.1766\n",
      "Epoch: 7, Batch: 1850/2500, Loss: 0.2859\n",
      "Epoch: 7, Batch: 1900/2500, Loss: 0.2234\n",
      "Epoch: 7, Batch: 1950/2500, Loss: 0.5802\n",
      "Epoch: 7, Batch: 2000/2500, Loss: 0.2137\n",
      "Epoch: 7, Batch: 2050/2500, Loss: 0.2186\n",
      "Epoch: 7, Batch: 2100/2500, Loss: 0.1415\n",
      "Epoch: 7, Batch: 2150/2500, Loss: 0.5150\n",
      "Epoch: 7, Batch: 2200/2500, Loss: 0.2027\n",
      "Epoch: 7, Batch: 2250/2500, Loss: 0.1928\n",
      "Epoch: 7, Batch: 2300/2500, Loss: 0.1944\n",
      "Epoch: 7, Batch: 2350/2500, Loss: 0.6372\n",
      "Epoch: 7, Batch: 2400/2500, Loss: 0.3262\n",
      "Epoch: 7, Batch: 2450/2500, Loss: 0.2840\n",
      "Epoch 7: Train Loss: 0.2710, Val Loss: 0.3575\n",
      "Epoch: 8, Batch: 0/2500, Loss: 0.0797\n",
      "Epoch: 8, Batch: 50/2500, Loss: 0.2464\n",
      "Epoch: 8, Batch: 100/2500, Loss: 0.3740\n",
      "Epoch: 8, Batch: 150/2500, Loss: 0.1852\n",
      "Epoch: 8, Batch: 200/2500, Loss: 0.1743\n",
      "Epoch: 8, Batch: 250/2500, Loss: 0.2448\n",
      "Epoch: 8, Batch: 300/2500, Loss: 0.1349\n",
      "Epoch: 8, Batch: 350/2500, Loss: 0.5028\n",
      "Epoch: 8, Batch: 400/2500, Loss: 0.3220\n",
      "Epoch: 8, Batch: 450/2500, Loss: 0.4166\n",
      "Epoch: 8, Batch: 500/2500, Loss: 0.4686\n",
      "Epoch: 8, Batch: 550/2500, Loss: 0.1983\n",
      "Epoch: 8, Batch: 600/2500, Loss: 0.1517\n",
      "Epoch: 8, Batch: 650/2500, Loss: 0.1905\n",
      "Epoch: 8, Batch: 700/2500, Loss: 0.2452\n",
      "Epoch: 8, Batch: 750/2500, Loss: 0.2835\n",
      "Epoch: 8, Batch: 800/2500, Loss: 0.4078\n",
      "Epoch: 8, Batch: 850/2500, Loss: 0.3195\n",
      "Epoch: 8, Batch: 900/2500, Loss: 0.2292\n",
      "Epoch: 8, Batch: 950/2500, Loss: 0.3083\n",
      "Epoch: 8, Batch: 1000/2500, Loss: 0.1795\n",
      "Epoch: 8, Batch: 1050/2500, Loss: 0.1414\n",
      "Epoch: 8, Batch: 1100/2500, Loss: 0.2004\n",
      "Epoch: 8, Batch: 1150/2500, Loss: 0.1067\n",
      "Epoch: 8, Batch: 1200/2500, Loss: 0.2098\n",
      "Epoch: 8, Batch: 1250/2500, Loss: 0.2561\n",
      "Epoch: 8, Batch: 1300/2500, Loss: 0.1353\n",
      "Epoch: 8, Batch: 1350/2500, Loss: 0.2761\n",
      "Epoch: 8, Batch: 1400/2500, Loss: 0.1798\n",
      "Epoch: 8, Batch: 1450/2500, Loss: 0.1830\n",
      "Epoch: 8, Batch: 1500/2500, Loss: 0.2760\n",
      "Epoch: 8, Batch: 1550/2500, Loss: 0.1403\n",
      "Epoch: 8, Batch: 1600/2500, Loss: 0.1142\n",
      "Epoch: 8, Batch: 1650/2500, Loss: 0.0523\n",
      "Epoch: 8, Batch: 1700/2500, Loss: 0.1201\n",
      "Epoch: 8, Batch: 1750/2500, Loss: 0.2205\n",
      "Epoch: 8, Batch: 1800/2500, Loss: 0.1782\n",
      "Epoch: 8, Batch: 1850/2500, Loss: 0.5295\n",
      "Epoch: 8, Batch: 1900/2500, Loss: 0.2131\n",
      "Epoch: 8, Batch: 1950/2500, Loss: 0.1129\n",
      "Epoch: 8, Batch: 2000/2500, Loss: 0.1596\n",
      "Epoch: 8, Batch: 2050/2500, Loss: 0.2163\n",
      "Epoch: 8, Batch: 2100/2500, Loss: 0.1329\n",
      "Epoch: 8, Batch: 2150/2500, Loss: 0.1733\n",
      "Epoch: 8, Batch: 2200/2500, Loss: 0.1721\n",
      "Epoch: 8, Batch: 2250/2500, Loss: 0.1740\n",
      "Epoch: 8, Batch: 2300/2500, Loss: 0.2056\n",
      "Epoch: 8, Batch: 2350/2500, Loss: 0.2147\n",
      "Epoch: 8, Batch: 2400/2500, Loss: 0.2160\n",
      "Epoch: 8, Batch: 2450/2500, Loss: 0.1190\n",
      "Epoch 8: Train Loss: 0.2206, Val Loss: 0.3429\n",
      "Epoch: 9, Batch: 0/2500, Loss: 0.3130\n",
      "Epoch: 9, Batch: 50/2500, Loss: 0.1118\n",
      "Epoch: 9, Batch: 100/2500, Loss: 0.1062\n",
      "Epoch: 9, Batch: 150/2500, Loss: 0.1408\n",
      "Epoch: 9, Batch: 200/2500, Loss: 0.3108\n",
      "Epoch: 9, Batch: 250/2500, Loss: 0.0778\n",
      "Epoch: 9, Batch: 300/2500, Loss: 0.1582\n",
      "Epoch: 9, Batch: 350/2500, Loss: 0.3837\n",
      "Epoch: 9, Batch: 400/2500, Loss: 0.1374\n",
      "Epoch: 9, Batch: 450/2500, Loss: 0.1126\n",
      "Epoch: 9, Batch: 500/2500, Loss: 0.1322\n",
      "Epoch: 9, Batch: 550/2500, Loss: 0.1040\n",
      "Epoch: 9, Batch: 600/2500, Loss: 0.1465\n",
      "Epoch: 9, Batch: 650/2500, Loss: 0.1194\n",
      "Epoch: 9, Batch: 700/2500, Loss: 0.1557\n",
      "Epoch: 9, Batch: 750/2500, Loss: 0.3620\n",
      "Epoch: 9, Batch: 800/2500, Loss: 0.3108\n",
      "Epoch: 9, Batch: 850/2500, Loss: 0.0948\n",
      "Epoch: 9, Batch: 900/2500, Loss: 0.1876\n",
      "Epoch: 9, Batch: 950/2500, Loss: 0.1093\n",
      "Epoch: 9, Batch: 1000/2500, Loss: 0.3301\n",
      "Epoch: 9, Batch: 1050/2500, Loss: 0.2130\n",
      "Epoch: 9, Batch: 1100/2500, Loss: 0.3084\n",
      "Epoch: 9, Batch: 1150/2500, Loss: 0.1546\n",
      "Epoch: 9, Batch: 1200/2500, Loss: 0.2603\n",
      "Epoch: 9, Batch: 1250/2500, Loss: 0.1929\n",
      "Epoch: 9, Batch: 1300/2500, Loss: 0.1599\n",
      "Epoch: 9, Batch: 1350/2500, Loss: 0.1057\n",
      "Epoch: 9, Batch: 1400/2500, Loss: 0.1957\n",
      "Epoch: 9, Batch: 1450/2500, Loss: 0.1800\n",
      "Epoch: 9, Batch: 1500/2500, Loss: 0.1779\n",
      "Epoch: 9, Batch: 1550/2500, Loss: 0.1367\n",
      "Epoch: 9, Batch: 1600/2500, Loss: 0.2078\n",
      "Epoch: 9, Batch: 1650/2500, Loss: 0.1294\n",
      "Epoch: 9, Batch: 1700/2500, Loss: 0.1238\n",
      "Epoch: 9, Batch: 1750/2500, Loss: 0.0782\n",
      "Epoch: 9, Batch: 1800/2500, Loss: 0.2821\n",
      "Epoch: 9, Batch: 1850/2500, Loss: 0.0934\n",
      "Epoch: 9, Batch: 1900/2500, Loss: 0.1669\n",
      "Epoch: 9, Batch: 1950/2500, Loss: 0.2307\n",
      "Epoch: 9, Batch: 2000/2500, Loss: 0.1626\n",
      "Epoch: 9, Batch: 2050/2500, Loss: 0.1811\n",
      "Epoch: 9, Batch: 2100/2500, Loss: 0.1093\n",
      "Epoch: 9, Batch: 2150/2500, Loss: 0.1705\n",
      "Epoch: 9, Batch: 2200/2500, Loss: 0.2584\n",
      "Epoch: 9, Batch: 2250/2500, Loss: 0.2666\n",
      "Epoch: 9, Batch: 2300/2500, Loss: 0.2063\n",
      "Epoch: 9, Batch: 2350/2500, Loss: 0.1423\n",
      "Epoch: 9, Batch: 2400/2500, Loss: 0.0975\n",
      "Epoch: 9, Batch: 2450/2500, Loss: 0.1925\n",
      "Epoch 9: Train Loss: 0.1850, Val Loss: 0.1917\n",
      "Saved best model with validation loss: 0.1917\n",
      "Epoch: 10, Batch: 0/2500, Loss: 0.2695\n",
      "Epoch: 10, Batch: 50/2500, Loss: 0.2276\n",
      "Epoch: 10, Batch: 100/2500, Loss: 0.3254\n",
      "Epoch: 10, Batch: 150/2500, Loss: 0.1966\n",
      "Epoch: 10, Batch: 200/2500, Loss: 0.1283\n",
      "Epoch: 10, Batch: 250/2500, Loss: 0.0585\n",
      "Epoch: 10, Batch: 300/2500, Loss: 0.1478\n",
      "Epoch: 10, Batch: 350/2500, Loss: 0.1228\n",
      "Epoch: 10, Batch: 400/2500, Loss: 0.2384\n",
      "Epoch: 10, Batch: 450/2500, Loss: 0.1888\n",
      "Epoch: 10, Batch: 500/2500, Loss: 0.0991\n",
      "Epoch: 10, Batch: 550/2500, Loss: 0.1969\n",
      "Epoch: 10, Batch: 600/2500, Loss: 0.1781\n",
      "Epoch: 10, Batch: 650/2500, Loss: 0.0959\n",
      "Epoch: 10, Batch: 700/2500, Loss: 0.0743\n",
      "Epoch: 10, Batch: 750/2500, Loss: 0.0879\n",
      "Epoch: 10, Batch: 800/2500, Loss: 0.0925\n",
      "Epoch: 10, Batch: 850/2500, Loss: 0.1848\n",
      "Epoch: 10, Batch: 900/2500, Loss: 0.1951\n",
      "Epoch: 10, Batch: 950/2500, Loss: 0.0945\n",
      "Epoch: 10, Batch: 1000/2500, Loss: 0.1348\n",
      "Epoch: 10, Batch: 1050/2500, Loss: 0.2110\n",
      "Epoch: 10, Batch: 1100/2500, Loss: 0.1320\n",
      "Epoch: 10, Batch: 1150/2500, Loss: 0.1626\n",
      "Epoch: 10, Batch: 1200/2500, Loss: 0.0948\n",
      "Epoch: 10, Batch: 1250/2500, Loss: 0.1145\n",
      "Epoch: 10, Batch: 1300/2500, Loss: 0.1641\n",
      "Epoch: 10, Batch: 1350/2500, Loss: 0.1515\n",
      "Epoch: 10, Batch: 1400/2500, Loss: 0.1342\n",
      "Epoch: 10, Batch: 1450/2500, Loss: 0.2317\n",
      "Epoch: 10, Batch: 1500/2500, Loss: 0.3101\n",
      "Epoch: 10, Batch: 1550/2500, Loss: 0.2193\n",
      "Epoch: 10, Batch: 1600/2500, Loss: 0.1368\n",
      "Epoch: 10, Batch: 1650/2500, Loss: 0.0704\n",
      "Epoch: 10, Batch: 1700/2500, Loss: 0.1061\n",
      "Epoch: 10, Batch: 1750/2500, Loss: 0.1592\n",
      "Epoch: 10, Batch: 1800/2500, Loss: 0.3303\n",
      "Epoch: 10, Batch: 1850/2500, Loss: 0.0942\n",
      "Epoch: 10, Batch: 1900/2500, Loss: 0.0893\n",
      "Epoch: 10, Batch: 1950/2500, Loss: 0.1193\n",
      "Epoch: 10, Batch: 2000/2500, Loss: 0.1053\n",
      "Epoch: 10, Batch: 2050/2500, Loss: 0.2257\n",
      "Epoch: 10, Batch: 2100/2500, Loss: 0.1736\n",
      "Epoch: 10, Batch: 2150/2500, Loss: 0.1327\n",
      "Epoch: 10, Batch: 2200/2500, Loss: 0.1346\n",
      "Epoch: 10, Batch: 2250/2500, Loss: 0.1148\n",
      "Epoch: 10, Batch: 2300/2500, Loss: 0.4884\n",
      "Epoch: 10, Batch: 2350/2500, Loss: 0.1086\n",
      "Epoch: 10, Batch: 2400/2500, Loss: 0.2882\n",
      "Epoch: 10, Batch: 2450/2500, Loss: 0.1331\n",
      "Epoch 10: Train Loss: 0.1557, Val Loss: 2.2481\n",
      "Epoch: 11, Batch: 0/2500, Loss: 0.1632\n",
      "Epoch: 11, Batch: 50/2500, Loss: 0.1150\n",
      "Epoch: 11, Batch: 100/2500, Loss: 0.0671\n",
      "Epoch: 11, Batch: 150/2500, Loss: 0.1344\n",
      "Epoch: 11, Batch: 200/2500, Loss: 0.1788\n",
      "Epoch: 11, Batch: 250/2500, Loss: 0.1031\n",
      "Epoch: 11, Batch: 300/2500, Loss: 0.1976\n",
      "Epoch: 11, Batch: 350/2500, Loss: 0.0823\n",
      "Epoch: 11, Batch: 400/2500, Loss: 0.1216\n",
      "Epoch: 11, Batch: 450/2500, Loss: 0.0565\n",
      "Epoch: 11, Batch: 500/2500, Loss: 0.0698\n",
      "Epoch: 11, Batch: 550/2500, Loss: 0.0935\n",
      "Epoch: 11, Batch: 600/2500, Loss: 0.1147\n",
      "Epoch: 11, Batch: 650/2500, Loss: 0.1325\n",
      "Epoch: 11, Batch: 700/2500, Loss: 0.0541\n",
      "Epoch: 11, Batch: 750/2500, Loss: 0.1252\n",
      "Epoch: 11, Batch: 800/2500, Loss: 0.1568\n",
      "Epoch: 11, Batch: 850/2500, Loss: 0.1272\n",
      "Epoch: 11, Batch: 900/2500, Loss: 0.1326\n",
      "Epoch: 11, Batch: 950/2500, Loss: 0.1782\n",
      "Epoch: 11, Batch: 1000/2500, Loss: 0.0692\n",
      "Epoch: 11, Batch: 1050/2500, Loss: 0.0951\n",
      "Epoch: 11, Batch: 1100/2500, Loss: 0.1470\n",
      "Epoch: 11, Batch: 1150/2500, Loss: 0.0922\n",
      "Epoch: 11, Batch: 1200/2500, Loss: 0.0633\n",
      "Epoch: 11, Batch: 1250/2500, Loss: 0.1277\n",
      "Epoch: 11, Batch: 1300/2500, Loss: 0.1544\n",
      "Epoch: 11, Batch: 1350/2500, Loss: 0.0606\n",
      "Epoch: 11, Batch: 1400/2500, Loss: 0.0604\n",
      "Epoch: 11, Batch: 1450/2500, Loss: 0.1460\n",
      "Epoch: 11, Batch: 1500/2500, Loss: 0.1231\n",
      "Epoch: 11, Batch: 1550/2500, Loss: 0.1597\n",
      "Epoch: 11, Batch: 1600/2500, Loss: 0.1335\n",
      "Epoch: 11, Batch: 1650/2500, Loss: 0.2017\n",
      "Epoch: 11, Batch: 1700/2500, Loss: 0.0906\n",
      "Epoch: 11, Batch: 1750/2500, Loss: 0.1375\n",
      "Epoch: 11, Batch: 1800/2500, Loss: 0.2383\n",
      "Epoch: 11, Batch: 1850/2500, Loss: 0.0818\n",
      "Epoch: 11, Batch: 1900/2500, Loss: 0.0422\n",
      "Epoch: 11, Batch: 1950/2500, Loss: 0.1048\n",
      "Epoch: 11, Batch: 2000/2500, Loss: 0.0549\n",
      "Epoch: 11, Batch: 2050/2500, Loss: 0.2946\n",
      "Epoch: 11, Batch: 2100/2500, Loss: 0.0555\n",
      "Epoch: 11, Batch: 2150/2500, Loss: 0.0678\n",
      "Epoch: 11, Batch: 2200/2500, Loss: 0.0968\n",
      "Epoch: 11, Batch: 2250/2500, Loss: 0.2011\n",
      "Epoch: 11, Batch: 2300/2500, Loss: 0.0929\n",
      "Epoch: 11, Batch: 2350/2500, Loss: 0.2015\n",
      "Epoch: 11, Batch: 2400/2500, Loss: 0.2444\n",
      "Epoch: 11, Batch: 2450/2500, Loss: 0.3628\n",
      "Epoch 11: Train Loss: 0.1340, Val Loss: 0.2132\n",
      "Epoch: 12, Batch: 0/2500, Loss: 0.0709\n",
      "Epoch: 12, Batch: 50/2500, Loss: 0.0774\n",
      "Epoch: 12, Batch: 100/2500, Loss: 0.0609\n",
      "Epoch: 12, Batch: 150/2500, Loss: 0.0867\n",
      "Epoch: 12, Batch: 200/2500, Loss: 0.1961\n",
      "Epoch: 12, Batch: 250/2500, Loss: 0.0983\n",
      "Epoch: 12, Batch: 300/2500, Loss: 0.1282\n",
      "Epoch: 12, Batch: 350/2500, Loss: 0.1023\n",
      "Epoch: 12, Batch: 400/2500, Loss: 0.0763\n",
      "Epoch: 12, Batch: 450/2500, Loss: 0.1207\n",
      "Epoch: 12, Batch: 500/2500, Loss: 0.0686\n",
      "Epoch: 12, Batch: 550/2500, Loss: 0.1053\n",
      "Epoch: 12, Batch: 600/2500, Loss: 0.0349\n",
      "Epoch: 12, Batch: 650/2500, Loss: 0.1631\n",
      "Epoch: 12, Batch: 700/2500, Loss: 0.1381\n",
      "Epoch: 12, Batch: 750/2500, Loss: 0.0682\n",
      "Epoch: 12, Batch: 800/2500, Loss: 0.0850\n",
      "Epoch: 12, Batch: 850/2500, Loss: 0.0590\n",
      "Epoch: 12, Batch: 900/2500, Loss: 0.1301\n",
      "Epoch: 12, Batch: 950/2500, Loss: 0.0819\n",
      "Epoch: 12, Batch: 1000/2500, Loss: 0.0556\n",
      "Epoch: 12, Batch: 1050/2500, Loss: 0.0934\n",
      "Epoch: 12, Batch: 1100/2500, Loss: 0.1423\n",
      "Epoch: 12, Batch: 1150/2500, Loss: 0.0356\n",
      "Epoch: 12, Batch: 1200/2500, Loss: 0.2903\n",
      "Epoch: 12, Batch: 1250/2500, Loss: 0.1099\n",
      "Epoch: 12, Batch: 1300/2500, Loss: 0.0781\n",
      "Epoch: 12, Batch: 1350/2500, Loss: 0.1524\n",
      "Epoch: 12, Batch: 1400/2500, Loss: 0.2165\n",
      "Epoch: 12, Batch: 1450/2500, Loss: 0.1120\n",
      "Epoch: 12, Batch: 1500/2500, Loss: 0.1274\n",
      "Epoch: 12, Batch: 1550/2500, Loss: 0.3196\n",
      "Epoch: 12, Batch: 1600/2500, Loss: 0.1107\n",
      "Epoch: 12, Batch: 1650/2500, Loss: 0.1431\n",
      "Epoch: 12, Batch: 1700/2500, Loss: 0.0648\n",
      "Epoch: 12, Batch: 1750/2500, Loss: 0.0653\n",
      "Epoch: 12, Batch: 1800/2500, Loss: 0.0627\n",
      "Epoch: 12, Batch: 1850/2500, Loss: 0.1116\n",
      "Epoch: 12, Batch: 1900/2500, Loss: 0.2056\n",
      "Epoch: 12, Batch: 1950/2500, Loss: 0.1394\n",
      "Epoch: 12, Batch: 2000/2500, Loss: 0.0944\n",
      "Epoch: 12, Batch: 2050/2500, Loss: 0.0560\n",
      "Epoch: 12, Batch: 2100/2500, Loss: 0.0794\n",
      "Epoch: 12, Batch: 2150/2500, Loss: 0.1063\n",
      "Epoch: 12, Batch: 2200/2500, Loss: 0.1242\n",
      "Epoch: 12, Batch: 2250/2500, Loss: 0.0976\n",
      "Epoch: 12, Batch: 2300/2500, Loss: 0.0457\n",
      "Epoch: 12, Batch: 2350/2500, Loss: 0.1641\n",
      "Epoch: 12, Batch: 2400/2500, Loss: 0.1367\n",
      "Epoch: 12, Batch: 2450/2500, Loss: 0.0522\n",
      "Epoch 12: Train Loss: 0.1183, Val Loss: 0.1964\n",
      "Epoch: 13, Batch: 0/2500, Loss: 0.1051\n",
      "Epoch: 13, Batch: 50/2500, Loss: 0.1283\n",
      "Epoch: 13, Batch: 100/2500, Loss: 0.0978\n",
      "Epoch: 13, Batch: 150/2500, Loss: 0.0844\n",
      "Epoch: 13, Batch: 200/2500, Loss: 0.0445\n",
      "Epoch: 13, Batch: 250/2500, Loss: 0.0264\n",
      "Epoch: 13, Batch: 300/2500, Loss: 0.1875\n",
      "Epoch: 13, Batch: 350/2500, Loss: 0.1016\n",
      "Epoch: 13, Batch: 400/2500, Loss: 0.0806\n",
      "Epoch: 13, Batch: 450/2500, Loss: 0.1979\n",
      "Epoch: 13, Batch: 500/2500, Loss: 0.0951\n",
      "Epoch: 13, Batch: 550/2500, Loss: 0.0626\n",
      "Epoch: 13, Batch: 600/2500, Loss: 0.0594\n",
      "Epoch: 13, Batch: 650/2500, Loss: 0.0677\n",
      "Epoch: 13, Batch: 700/2500, Loss: 0.2666\n",
      "Epoch: 13, Batch: 750/2500, Loss: 0.0754\n",
      "Epoch: 13, Batch: 800/2500, Loss: 0.0644\n",
      "Epoch: 13, Batch: 850/2500, Loss: 0.0639\n",
      "Epoch: 13, Batch: 900/2500, Loss: 0.0385\n",
      "Epoch: 13, Batch: 950/2500, Loss: 0.0700\n",
      "Epoch: 13, Batch: 1000/2500, Loss: 0.1497\n",
      "Epoch: 13, Batch: 1050/2500, Loss: 0.1012\n",
      "Epoch: 13, Batch: 1100/2500, Loss: 0.1333\n",
      "Epoch: 13, Batch: 1150/2500, Loss: 0.0848\n",
      "Epoch: 13, Batch: 1200/2500, Loss: 0.1281\n",
      "Epoch: 13, Batch: 1250/2500, Loss: 0.1474\n",
      "Epoch: 13, Batch: 1300/2500, Loss: 0.0667\n",
      "Epoch: 13, Batch: 1350/2500, Loss: 0.0847\n",
      "Epoch: 13, Batch: 1400/2500, Loss: 0.1259\n",
      "Epoch: 13, Batch: 1450/2500, Loss: 0.1508\n",
      "Epoch: 13, Batch: 1500/2500, Loss: 0.0627\n",
      "Epoch: 13, Batch: 1550/2500, Loss: 0.1949\n",
      "Epoch: 13, Batch: 1600/2500, Loss: 0.1114\n",
      "Epoch: 13, Batch: 1650/2500, Loss: 0.0889\n",
      "Epoch: 13, Batch: 1700/2500, Loss: 0.0278\n",
      "Epoch: 13, Batch: 1750/2500, Loss: 0.0989\n",
      "Epoch: 13, Batch: 1800/2500, Loss: 0.0471\n",
      "Epoch: 13, Batch: 1850/2500, Loss: 0.0716\n",
      "Epoch: 13, Batch: 1900/2500, Loss: 0.0330\n",
      "Epoch: 13, Batch: 1950/2500, Loss: 0.0960\n",
      "Epoch: 13, Batch: 2000/2500, Loss: 0.2374\n",
      "Epoch: 13, Batch: 2050/2500, Loss: 0.1279\n",
      "Epoch: 13, Batch: 2100/2500, Loss: 0.0938\n",
      "Epoch: 13, Batch: 2150/2500, Loss: 0.1526\n",
      "Epoch: 13, Batch: 2200/2500, Loss: 0.1176\n",
      "Epoch: 13, Batch: 2250/2500, Loss: 0.1456\n",
      "Epoch: 13, Batch: 2300/2500, Loss: 0.1141\n",
      "Epoch: 13, Batch: 2350/2500, Loss: 0.0456\n",
      "Epoch: 13, Batch: 2400/2500, Loss: 0.0886\n",
      "Epoch: 13, Batch: 2450/2500, Loss: 0.2365\n",
      "Epoch 13: Train Loss: 0.1033, Val Loss: 0.1750\n",
      "Saved best model with validation loss: 0.1750\n",
      "Epoch: 14, Batch: 0/2500, Loss: 0.1382\n",
      "Epoch: 14, Batch: 50/2500, Loss: 0.1437\n",
      "Epoch: 14, Batch: 100/2500, Loss: 0.0409\n",
      "Epoch: 14, Batch: 150/2500, Loss: 0.1145\n",
      "Epoch: 14, Batch: 200/2500, Loss: 0.0703\n",
      "Epoch: 14, Batch: 250/2500, Loss: 0.1425\n",
      "Epoch: 14, Batch: 300/2500, Loss: 0.0626\n",
      "Epoch: 14, Batch: 350/2500, Loss: 0.0914\n",
      "Epoch: 14, Batch: 400/2500, Loss: 0.0506\n",
      "Epoch: 14, Batch: 450/2500, Loss: 0.1214\n",
      "Epoch: 14, Batch: 500/2500, Loss: 0.1316\n",
      "Epoch: 14, Batch: 550/2500, Loss: 0.0990\n",
      "Epoch: 14, Batch: 600/2500, Loss: 0.0288\n",
      "Epoch: 14, Batch: 650/2500, Loss: 0.1131\n",
      "Epoch: 14, Batch: 700/2500, Loss: 0.0525\n",
      "Epoch: 14, Batch: 750/2500, Loss: 0.0966\n",
      "Epoch: 14, Batch: 800/2500, Loss: 0.1401\n",
      "Epoch: 14, Batch: 850/2500, Loss: 0.0516\n",
      "Epoch: 14, Batch: 900/2500, Loss: 0.1269\n",
      "Epoch: 14, Batch: 950/2500, Loss: 0.0760\n",
      "Epoch: 14, Batch: 1000/2500, Loss: 0.0620\n",
      "Epoch: 14, Batch: 1050/2500, Loss: 0.0816\n",
      "Epoch: 14, Batch: 1100/2500, Loss: 0.0847\n",
      "Epoch: 14, Batch: 1150/2500, Loss: 0.0390\n",
      "Epoch: 14, Batch: 1200/2500, Loss: 0.2399\n",
      "Epoch: 14, Batch: 1250/2500, Loss: 0.0601\n",
      "Epoch: 14, Batch: 1300/2500, Loss: 0.1053\n",
      "Epoch: 14, Batch: 1350/2500, Loss: 0.1298\n",
      "Epoch: 14, Batch: 1400/2500, Loss: 0.0534\n",
      "Epoch: 14, Batch: 1450/2500, Loss: 0.1433\n",
      "Epoch: 14, Batch: 1500/2500, Loss: 0.0541\n",
      "Epoch: 14, Batch: 1550/2500, Loss: 0.0909\n",
      "Epoch: 14, Batch: 1600/2500, Loss: 0.1044\n",
      "Epoch: 14, Batch: 1650/2500, Loss: 0.1903\n",
      "Epoch: 14, Batch: 1700/2500, Loss: 0.0509\n",
      "Epoch: 14, Batch: 1750/2500, Loss: 0.0876\n",
      "Epoch: 14, Batch: 1800/2500, Loss: 0.1351\n",
      "Epoch: 14, Batch: 1850/2500, Loss: 0.0453\n",
      "Epoch: 14, Batch: 1900/2500, Loss: 0.0739\n",
      "Epoch: 14, Batch: 1950/2500, Loss: 0.0516\n",
      "Epoch: 14, Batch: 2000/2500, Loss: 0.1151\n",
      "Epoch: 14, Batch: 2050/2500, Loss: 0.0592\n",
      "Epoch: 14, Batch: 2100/2500, Loss: 0.2260\n",
      "Epoch: 14, Batch: 2150/2500, Loss: 0.0191\n",
      "Epoch: 14, Batch: 2200/2500, Loss: 0.0156\n",
      "Epoch: 14, Batch: 2250/2500, Loss: 0.2191\n",
      "Epoch: 14, Batch: 2300/2500, Loss: 0.0618\n",
      "Epoch: 14, Batch: 2350/2500, Loss: 0.0889\n",
      "Epoch: 14, Batch: 2400/2500, Loss: 0.0541\n",
      "Epoch: 14, Batch: 2450/2500, Loss: 0.0593\n",
      "Epoch 14: Train Loss: 0.0944, Val Loss: 2.5423\n",
      "Epoch: 15, Batch: 0/2500, Loss: 0.1691\n",
      "Epoch: 15, Batch: 50/2500, Loss: 0.1077\n",
      "Epoch: 15, Batch: 100/2500, Loss: 0.1108\n",
      "Epoch: 15, Batch: 150/2500, Loss: 0.0609\n",
      "Epoch: 15, Batch: 200/2500, Loss: 0.0366\n",
      "Epoch: 15, Batch: 250/2500, Loss: 0.0637\n",
      "Epoch: 15, Batch: 300/2500, Loss: 0.0290\n",
      "Epoch: 15, Batch: 350/2500, Loss: 0.0174\n",
      "Epoch: 15, Batch: 400/2500, Loss: 0.0383\n",
      "Epoch: 15, Batch: 450/2500, Loss: 0.1160\n",
      "Epoch: 15, Batch: 500/2500, Loss: 0.0282\n",
      "Epoch: 15, Batch: 550/2500, Loss: 0.0540\n",
      "Epoch: 15, Batch: 600/2500, Loss: 0.0841\n",
      "Epoch: 15, Batch: 650/2500, Loss: 0.1097\n",
      "Epoch: 15, Batch: 700/2500, Loss: 0.1127\n",
      "Epoch: 15, Batch: 750/2500, Loss: 0.1374\n",
      "Epoch: 15, Batch: 800/2500, Loss: 0.0300\n",
      "Epoch: 15, Batch: 850/2500, Loss: 0.0848\n",
      "Epoch: 15, Batch: 900/2500, Loss: 0.0658\n",
      "Epoch: 15, Batch: 950/2500, Loss: 0.1083\n",
      "Epoch: 15, Batch: 1000/2500, Loss: 0.0299\n",
      "Epoch: 15, Batch: 1050/2500, Loss: 0.0802\n",
      "Epoch: 15, Batch: 1100/2500, Loss: 0.0589\n",
      "Epoch: 15, Batch: 1150/2500, Loss: 0.1414\n",
      "Epoch: 15, Batch: 1200/2500, Loss: 0.0432\n",
      "Epoch: 15, Batch: 1250/2500, Loss: 0.0385\n",
      "Epoch: 15, Batch: 1300/2500, Loss: 0.0611\n",
      "Epoch: 15, Batch: 1350/2500, Loss: 0.1020\n",
      "Epoch: 15, Batch: 1400/2500, Loss: 0.1730\n",
      "Epoch: 15, Batch: 1450/2500, Loss: 0.0921\n",
      "Epoch: 15, Batch: 1500/2500, Loss: 0.0495\n",
      "Epoch: 15, Batch: 1550/2500, Loss: 0.0940\n",
      "Epoch: 15, Batch: 1600/2500, Loss: 0.0947\n",
      "Epoch: 15, Batch: 1650/2500, Loss: 0.1099\n",
      "Epoch: 15, Batch: 1700/2500, Loss: 0.0603\n",
      "Epoch: 15, Batch: 1750/2500, Loss: 0.1103\n",
      "Epoch: 15, Batch: 1800/2500, Loss: 0.0551\n",
      "Epoch: 15, Batch: 1850/2500, Loss: 0.1578\n",
      "Epoch: 15, Batch: 1900/2500, Loss: 0.2069\n",
      "Epoch: 15, Batch: 1950/2500, Loss: 0.1670\n",
      "Epoch: 15, Batch: 2000/2500, Loss: 0.0584\n",
      "Epoch: 15, Batch: 2050/2500, Loss: 0.0436\n",
      "Epoch: 15, Batch: 2100/2500, Loss: 0.0283\n",
      "Epoch: 15, Batch: 2150/2500, Loss: 0.1178\n",
      "Epoch: 15, Batch: 2200/2500, Loss: 0.2562\n",
      "Epoch: 15, Batch: 2250/2500, Loss: 0.1289\n",
      "Epoch: 15, Batch: 2300/2500, Loss: 0.2525\n",
      "Epoch: 15, Batch: 2350/2500, Loss: 0.0195\n",
      "Epoch: 15, Batch: 2400/2500, Loss: 0.0835\n",
      "Epoch: 15, Batch: 2450/2500, Loss: 0.0141\n",
      "Epoch 15: Train Loss: 0.0845, Val Loss: 0.3006\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not found, using CPU\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataset = OCRDataset(\n",
    "    image_dir=\"../../data/interim/5/OCR_dataset/\",\n",
    "    word_list_path=\"words.txt\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Print dataset size\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "model = OCRModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "num_epochs = 15\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, batch_losses = train_model(\n",
    "        model, train_loader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, lengths in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for t in range(MAX_WORD_LENGTH):\n",
    "                val_loss += criterion(outputs[:, t, :], labels[:, t]).item()\n",
    "    \n",
    "    val_loss /= len(val_loader) * MAX_WORD_LENGTH\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, 'best_ocr_model_1.pth')\n",
    "        print(f'Saved best model with validation loss: {val_loss:.4f}')\n",
    "    \n",
    "    plot_training_progress(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch-wise train and validation losses:   \n",
    "   \n",
    "\n",
    "\n",
    "Epoch 1: Train Loss: 1.3782, Val Loss: 1.3801  \n",
    "Saved best model with validation loss: 1.3801  \n",
    "\n",
    "Epoch 2: Train Loss: 1.1173, Val Loss: 1.0039  \n",
    "Saved best model with validation loss: 1.0039  \n",
    "\n",
    "Epoch 3: Train Loss: 0.9307, Val Loss: 0.7896  \n",
    "Saved best model with validation loss: 0.7896  \n",
    "\n",
    "Epoch 4: Train Loss: 0.6743, Val Loss: 0.6508  \n",
    "Saved best model with validation loss: 0.6508  \n",
    "\n",
    "Epoch 5: Train Loss: 0.4581, Val Loss: 0.4033  \n",
    "Saved best model with validation loss: 0.4033  \n",
    "\n",
    "Epoch 6: Train Loss: 0.3439, Val Loss: 0.2867  \n",
    "Saved best model with validation loss: 0.2867  \n",
    "\n",
    "Epoch 7: Train Loss: 0.2710, Val Loss: 0.3575  \n",
    "\n",
    "Epoch 8: Train Loss: 0.2206, Val Loss: 0.3429  \n",
    "\n",
    "Epoch 9: Train Loss: 0.1850, Val Loss: 0.1917  \n",
    "Saved best model with validation loss: 0.1917  \n",
    "\n",
    "Epoch 10: Train Loss: 0.1557, Val Loss: 2.2481  \n",
    "\n",
    "Epoch 11: Train Loss: 0.1340, Val Loss: 0.2132  \n",
    "\n",
    "Epoch 12: Train Loss: 0.1183, Val Loss: 0.1964  \n",
    "\n",
    "Epoch 13: Train Loss: 0.1033, Val Loss: 0.1750  \n",
    "Saved best model with validation loss: 0.1750  \n",
    "\n",
    "Epoch 14: Train Loss: 0.0944, Val Loss: 2.5423  \n",
    "\n",
    "Epoch 15: Train Loss: 0.0845, Val Loss: 0.3006  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "\n",
      "Loading best model from checkpoint...\n",
      "Model loaded from epoch 13\n",
      "Previous training metrics:\n",
      "- Training loss: 0.1033\n",
      "- Validation loss: 0.1750\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Model Evaluation Results:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9s/y0c_yj6d7c1fm05rdc4dnn7c0000gn/T/ipykernel_2877/2292109605.py:210: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_ocr_model_1.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy (Average Correct Characters): 0.9619\n",
      "Random Baseline Accuracy: 0.0381\n",
      "Improvement over baseline: 0.9238\n",
      "\n",
      "Example Predictions:\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 1:\n",
      "True word:      considerability\n",
      "Predicted word: considerabilityyeeyy\n",
      "Correct characters: 15/15\n",
      "\n",
      "Example 2:\n",
      "True word:      extratympanic\n",
      "Predicted word: extratympaniccyyee\n",
      "Correct characters: 13/13\n",
      "\n",
      "Example 3:\n",
      "True word:      bewinged\n",
      "Predicted word: bewingeddyyeeyy\n",
      "Correct characters: 8/8\n",
      "\n",
      "Example 4:\n",
      "True word:      hipless\n",
      "Predicted word: hiplessyyeeyy\n",
      "Correct characters: 7/7\n",
      "\n",
      "Example 5:\n",
      "True word:      cosmographer\n",
      "Predicted word: cosmographeryyeeyy\n",
      "Correct characters: 12/12\n",
      "\n",
      "Example 6:\n",
      "True word:      flagroot\n",
      "Predicted word: flagroottyyeeyy\n",
      "Correct characters: 8/8\n",
      "\n",
      "Example 7:\n",
      "True word:      filaria\n",
      "Predicted word: filariaaeyyeeyy\n",
      "Correct characters: 7/7\n",
      "\n",
      "Example 8:\n",
      "True word:      hemispherically\n",
      "Predicted word: hemisphericallyyeeyy\n",
      "Correct characters: 15/15\n",
      "\n",
      "Example 9:\n",
      "True word:      inconstancy\n",
      "Predicted word: inconstancyyeeyy\n",
      "Correct characters: 11/11\n",
      "\n",
      "Example 10:\n",
      "True word:      amchoor\n",
      "Predicted word: amchoorryyeeyy\n",
      "Correct characters: 7/7\n",
      "\n",
      "Example 11:\n",
      "True word:      abacinate\n",
      "Predicted word: abacinateeyyeeyy\n",
      "Correct characters: 9/9\n",
      "\n",
      "Example 12:\n",
      "True word:      blancard\n",
      "Predicted word: blancarddyyeeyy\n",
      "Correct characters: 8/8\n",
      "\n",
      "Example 13:\n",
      "True word:      every\n",
      "Predicted word: everyyeeyy\n",
      "Correct characters: 5/5\n",
      "\n",
      "Example 14:\n",
      "True word:      abstentionist\n",
      "Predicted word: abstenttionsstyyeeyy\n",
      "Correct characters: 8/13\n",
      "\n",
      "Example 15:\n",
      "True word:      hemolymphatic\n",
      "Predicted word: hemocyyphaticcyyeeyy\n",
      "Correct characters: 11/13\n",
      "\n",
      "Example 16:\n",
      "True word:      disthrall\n",
      "Predicted word: disthrallyyeeyy\n",
      "Correct characters: 9/9\n",
      "\n",
      "Example 17:\n",
      "True word:      disbursement\n",
      "Predicted word: disbursementtyyeeyy\n",
      "Correct characters: 12/12\n",
      "\n",
      "Example 18:\n",
      "True word:      avouchment\n",
      "Predicted word: avonchmenttyyeeyy\n",
      "Correct characters: 9/10\n",
      "\n",
      "Example 19:\n",
      "True word:      grasshouse\n",
      "Predicted word: grasshouseeyyeeyy\n",
      "Correct characters: 10/10\n",
      "\n",
      "Example 20:\n",
      "True word:      aspredo\n",
      "Predicted word: aspredooryyeeyy\n",
      "Correct characters: 7/7\n",
      "\n",
      "Final Results Summary:\n",
      "--------------------------------------------------\n",
      "Model Accuracy: 0.9619\n",
      "Baseline Accuracy: 0.0381\n",
      "Improvement over baseline: 0.9238\n",
      "\n",
      "Example predictions have been plotted and saved as 'example_predictions.png'\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, targets, lengths):\n",
    "    batch_size = predictions.size(0)\n",
    "    pred_chars = predictions.argmax(dim=2)  # Convert from one-hot to indices\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        length = lengths[i]\n",
    "        correct_chars += (pred_chars[i, :length] == targets[i, :length]).sum().item()\n",
    "        total_chars += length\n",
    "    \n",
    "    return correct_chars / total_chars if total_chars > 0 else 0\n",
    "\n",
    "def generate_random_baseline(batch_size, max_length, vocab_size):\n",
    "    return torch.randint(1, vocab_size, (batch_size, max_length))\n",
    "\n",
    "def clean_prediction(word):\n",
    "    # Remove non-alphabetic characters\n",
    "    word = ''.join(c for c in word if c.isalpha())\n",
    "    \n",
    "    if not word:\n",
    "        return word\n",
    "        \n",
    "    # Initialize result with first character\n",
    "    result = [word[0]]\n",
    "    \n",
    "    # Track repeated characters\n",
    "    repeat_count = 1\n",
    "    max_repeats = 2  \n",
    "    \n",
    "    # Process rest of the word\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] == result[-1]:\n",
    "            repeat_count += 1\n",
    "            if repeat_count <= max_repeats:\n",
    "                result.append(word[i])\n",
    "        else:\n",
    "            repeat_count = 1\n",
    "            result.append(word[i])\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "def decode_prediction(pred_tensor):\n",
    "    # First decode the tensor to a string\n",
    "    word = \"\"\n",
    "    for idx in pred_tensor:\n",
    "        if idx.item() == 0:  # Skip padding\n",
    "            break\n",
    "        if idx.item() in IDX_TO_CHAR:\n",
    "            word += IDX_TO_CHAR[idx.item()]\n",
    "    \n",
    "    # Clean up the prediction\n",
    "    return clean_prediction(word)\n",
    "\n",
    "def evaluate_model(model, val_loader, device, num_examples=20):\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    total_baseline_accuracy = 0\n",
    "    num_batches = 0\n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, lengths in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Model predictions\n",
    "            outputs = model(images)\n",
    "            pred_chars = outputs.argmax(dim=2)\n",
    "            accuracy = calculate_accuracy(outputs, labels, lengths)\n",
    "            \n",
    "            # Random baseline\n",
    "            random_preds = generate_random_baseline(\n",
    "                labels.size(0), \n",
    "                MAX_WORD_LENGTH, \n",
    "                VOCAB_SIZE\n",
    "            ).to(device)\n",
    "            baseline_accuracy = calculate_accuracy(\n",
    "                torch.nn.functional.one_hot(random_preds, VOCAB_SIZE).float(), \n",
    "                labels, \n",
    "                lengths\n",
    "            )\n",
    "            \n",
    "            total_accuracy += accuracy\n",
    "            total_baseline_accuracy += baseline_accuracy\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Collect example predictions\n",
    "            if len(examples) < num_examples:\n",
    "                for i in range(min(num_examples - len(examples), len(lengths))):\n",
    "                    true_word = decode_prediction(labels[i])\n",
    "                    pred_word = decode_prediction(pred_chars[i])\n",
    "                    examples.append({\n",
    "                        'true': true_word,\n",
    "                        'predicted': pred_word,\n",
    "                        'correct_chars': sum(1 for t, p in zip(true_word, pred_word) if t == p)\n",
    "                    })\n",
    "    \n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    avg_baseline_accuracy = total_baseline_accuracy / num_batches\n",
    "    \n",
    "    return avg_accuracy, avg_baseline_accuracy, examples\n",
    "\n",
    "def print_evaluation_results(model, val_loader, device):\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    accuracy, baseline_accuracy, examples = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Model Accuracy (Average Correct Characters): {accuracy:.4f}\")\n",
    "    print(f\"Random Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "    print(f\"Improvement over baseline: {accuracy - baseline_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nExample Predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"True word:      {example['true']}\")\n",
    "        print(f\"Predicted word: {example['predicted']}\")\n",
    "        print(f\"Correct characters: {example['correct_chars']}/{len(example['true'])}\")\n",
    "    \n",
    "    return accuracy, baseline_accuracy, examples\n",
    "\n",
    "\n",
    "\n",
    "def load_and_evaluate_model():\n",
    "    # Set device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"MPS device not found, using CPU\")\n",
    "    \n",
    "    \n",
    "    model = OCRModel().to(device)\n",
    "    \n",
    "    # Load the trained model\n",
    "    print(\"\\nLoading best model from checkpoint...\")\n",
    "    checkpoint = torch.load('best_ocr_model_1.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"Model loaded from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"Previous training metrics:\")\n",
    "    print(f\"- Training loss: {checkpoint['train_loss']:.4f}\")\n",
    "    print(f\"- Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    # Prepare dataset and loader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    dataset = OCRDataset(\n",
    "        image_dir=\"../../data/interim/5/OCR_dataset/\",\n",
    "        word_list_path=\"words.txt\",\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    _, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    model.eval()\n",
    "    accuracy, baseline_accuracy, examples = print_evaluation_results(model, val_loader, device)\n",
    "    \n",
    "    # Plot some example images with predictions\n",
    "    plot_example_predictions(model, val_loader, device)\n",
    "    \n",
    "    return model, accuracy, baseline_accuracy, examples\n",
    "\n",
    "def plot_example_predictions(model, val_loader, device, num_examples=5):\n",
    "    model.eval()\n",
    "    plt.figure(figsize=(15, 3*num_examples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images, labels, lengths = next(iter(val_loader))\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        pred_chars = outputs.argmax(dim=2)\n",
    "        \n",
    "        for i in range(min(num_examples, len(images))):\n",
    "            plt.subplot(num_examples, 1, i+1)\n",
    "            \n",
    "            # Convert tensor to image\n",
    "            img = images[i].cpu().squeeze().numpy()\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            \n",
    "            # Get true and predicted words\n",
    "            true_word = decode_prediction(labels[i])\n",
    "            pred_word = decode_prediction(pred_chars[i])\n",
    "            \n",
    "            plt.title(f'True: \"{true_word}\" | Predicted: \"{pred_word}\"')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('example_predictions.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and evaluate the model\n",
    "    model, accuracy, baseline_accuracy, examples = load_and_evaluate_model()\n",
    "    \n",
    "    print(\"\\nFinal Results Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "    print(f\"Improvement over baseline: {accuracy - baseline_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nExample predictions have been plotted and saved as 'example_predictions.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/example_predictions.png\" alt=\"example_predictions\" width=\"800\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
